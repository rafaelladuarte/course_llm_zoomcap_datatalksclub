## ğŸ§  Parte 1 â€“ IntroduÃ§Ã£o aos LLMs e ao RAG

### ğŸ“Œ Objetivo do Curso

* Curso prÃ¡tico focado em **LLMs (Modelos de Linguagem de Grande Escala)** e **RAG (Retrieval-Augmented Generation)**.
* Projeto principal: construir um sistema de perguntas e respostas baseado em documentos de FAQ dos cursos da comunidade DataTalks.Club.
* Problema: documentos FAQ extensos (ex. 321 pÃ¡ginas) sÃ£o difÃ­ceis de navegar manualmente.


### ğŸ¤– O que sÃ£o LLMs?

* **LLM = Large Language Model** (Modelo de Linguagem de Grande Escala).
* Treinados com grandes volumes de texto e bilhÃµes de parÃ¢metros.
* Principal tarefa: prever a prÃ³xima palavra/token com base no contexto anterior.
* Exemplo: sugestÃµes automÃ¡ticas de texto no celular.
* Comportamento sofisticado que simula conversa humana (ex: ChatGPT).
* Neste curso, os LLMs serÃ£o usados como **caixas-pretas** (sem aprofundar na teoria).


### âœï¸ Como funciona um LLM na prÃ¡tica?

* Entrada: um **prompt** (ex: pergunta do aluno).
* SaÃ­da: uma **resposta gerada** pelo modelo.
* Exemplo de prompt:

  ```
  Pergunta: Como me inscrevo no curso?
  Contexto: [texto extraÃ­do dos documentos FAQ]
  Resposta: [gerada pelo LLM]
  ```
![alt text](image-1.png)

### ğŸ” O que Ã© RAG (Retrieval-Augmented Generation)?

* **RAG = RecuperaÃ§Ã£o + GeraÃ§Ã£o**.
* Combina:

  * **RecuperaÃ§Ã£o de informaÃ§Ãµes** (busca em base de conhecimento);
  * **GeraÃ§Ã£o de texto** com LLM.
* RAG Ã© usado quando o LLM **nÃ£o tem a informaÃ§Ã£o na base de treino**.
* Exemplo:

  * Pergunta: â€œÃ‰ tarde demais para entrar no curso?â€
  * LLM puro: resposta vaga ou errada.
  * Com RAG: busca em documentos FAQ e gera resposta com base no conteÃºdo real.

### ğŸ§± Arquitetura de um sistema RAG

1. **UsuÃ¡rio faz uma pergunta (Q)**.
2. O sistema envia a pergunta para uma **base de conhecimento (FAQ, documentos)**.
3. Recupera os documentos mais relevantes (ex: D1, D2... D5).
4. Esses documentos sÃ£o usados como **contexto**.
5. O **LLM gera uma resposta** com base nesse contexto.

![](image.png)
### ğŸ› ï¸ O que serÃ¡ construÃ­do no curso

* Um sistema simples com formulÃ¡rio:

  * UsuÃ¡rio digita a pergunta.
  * O sistema responde com base nas FAQs usando RAG.
* Ferramentas: LLMs + TÃ©cnicas de recuperaÃ§Ã£o de informaÃ§Ãµes.

---

## ğŸ“¦ Parte 2 â€” Configurando o Ambiente

### ğŸ› ï¸ OpÃ§Ãµes de Ambiente

* O instrutor usa **GitHub Codespaces**, mas:

  * VocÃª **pode usar localmente** (com Docker instalado),
  * Ou plataformas como **Google Colab**, **AWS SageMaker**, **Cocalc** etc.
* Neste mÃ³dulo, **nÃ£o Ã© necessÃ¡rio GPU**, apenas no prÃ³ximo.

### ğŸš€ Usando GitHub Codespaces

1. **Criar repositÃ³rio GitHub**:

   * Nome sugerido: `llm-zoomcamp`
   * Deve ser **pÃºblico** (para envio das tarefas)
   * `.gitignore`: escolha `Python`
2. **Abrir com Codespaces**:

   * Clicar em **Code > Codespaces > Create Codespace**
   * Ambiente roda no navegador com **VS Code**
   * SugestÃ£o: abrir tambÃ©m no **VS Code Desktop** (melhor experiÃªncia)
   * Instalar extensÃ£o **Codespaces** no VS Code

### ğŸ Verificando o ambiente

* Docker e Docker Compose jÃ¡ disponÃ­veis
* Python prÃ©-instalado

### ğŸ“š InstalaÃ§Ã£o de Bibliotecas com `pip`

```bash
pip install jupyter==7.1.2 openai elasticsearch pandas
```

* `jupyter==7.1.2`: interface de notebooks
* `openai`: API da OpenAI
* `elasticsearch`: usado no fim do mÃ³dulo
* `pandas`: manipulaÃ§Ã£o de dados
* `tqdm`: barra de progresso (usado mais tarde)

### ğŸ”‘ ConfiguraÃ§Ã£o da API da OpenAI

1. Acesse: [platform.openai.com](https://platform.openai.com)
2. VÃ¡ em **API Keys** > Crie uma chave
3. **Nunca compartilhe** a chave (seguranÃ§a!)
4. Armazene como **variÃ¡vel de ambiente**:

```bash
export OPENAI_API_KEY="sua-chave-aqui"
```

### ğŸ§ª Testando o ambiente com o Jupyter

1. Rode o Jupyter Notebook:

```bash
jupyter notebook
```

2. Acesse via navegador: `localhost:8888`
3. Teste simples:

```python
import openai

client = openai.OpenAI(api_key="SUA_CHAVE")

response = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Ã‰ tarde demais para entrar no curso?"}]
)

print(response.choices[0].message.content)
```

### ğŸ’¡ Alternativa: Usando Anaconda

* Instale com:

```bash
wget https://repo.anaconda.com/archive/Anaconda3-<versÃ£o>-Linux-x86_64.sh
bash Anaconda3-<versÃ£o>-Linux-x86_64.sh
```

* Mais pesado (\~1GB), mas jÃ¡ inclui vÃ¡rias libs Ãºteis
* NÃ£o Ã© necessÃ¡rio instalar libs manualmente apÃ³s

Claro! Aqui estÃ¡ o **resumo em tÃ³picos** da **Parte 3** do vÃ­deo "[LLM Zoomcamp 1.3 - Retrieval and Search](https://www.youtube.com/watch?v=olvem333Bqo)" com base na transcriÃ§Ã£o:

## ğŸ§  Parte 3 - RecuperaÃ§Ã£o e Pesquisa

#### ğŸ“Œ VisÃ£o Geral

* RevisÃ£o da arquitetura **RAG (Retrieval-Augmented Generation)**:

  * Dois componentes principais: **Banco de Dados (mecanismo de busca)** e **LLM**.
* Nesta etapa: foco em **recuperar documentos relevantes** usando um mecanismo de busca simples antes de enviar para o LLM.

### ğŸ› ï¸ ImplementaÃ§Ã£o do Mecanismo de Busca

#### ğŸ”¹ Workshop anterior

* Baseado no workshop â€œImplementando um mecanismo de buscaâ€ (repositÃ³rio do curso).
* Mecanismo em memÃ³ria, **nÃ£o pronto para produÃ§Ã£o**, apenas para fins educacionais.

#### ğŸ”¹ Setup do mecanismo

* Uso de um script Python (`search.py`) baixado via `wget`.
* Carregamento dos dados de FAQ em formato JSON.

### ğŸ“ PreparaÃ§Ã£o dos Dados

* Cada item do JSON representa um curso (ex: ML Zoomcamp) com documentos internos.
* TransformaÃ§Ã£o dos dados:

  * Estrutura simplificada para uma **lista plana de documentos**.
  * AdiÃ§Ã£o do campo `course` para identificaÃ§Ã£o.

### ğŸ” IndexaÃ§Ã£o

* CriaÃ§Ã£o de um Ã­ndice com a funÃ§Ã£o `fit()` (parecido com o Scikit-learn).
* **Tipos de campos:**

  * `text_fields`: usados para **similaridade semÃ¢ntica** (ex: `question`, `section`).
  * `keyword_fields`: usados para **filtros exatos** (ex: `course`).


### ğŸ” Realizando Buscas

#### ğŸ“Œ Exemplo de busca:

> "O curso jÃ¡ comeÃ§ou, ainda posso me inscrever?"

* ParÃ¢metros da busca:

  * **Query**: a pergunta do usuÃ¡rio.
  * **Boost**:

    * Aumenta a relevÃ¢ncia de certos campos (ex: `question` com peso 3).
  * **Top\_k**: nÃºmero de resultados retornados.
  * **Filter**: restringe por curso (ex: apenas `Data Engineering Zoomcamp`).

#### âœ… Resultado

* RecuperaÃ§Ã£o de documentos relevantes, mesmo com variaÃ§Ãµes semÃ¢nticas ("participar" â‰  "inscrever-se").
* Filtro aplicado para garantir que os documentos sejam do curso correto.

### ğŸ’¡ ConclusÃ£o

* Base de conhecimento foi indexada com sucesso.
* Agora Ã© possÃ­vel recuperar documentos relevantes a partir de uma consulta.
* PrÃ³ximo passo: **inserir os documentos em um LLM** para gerar respostas contextualizadas (prÃ³ximo vÃ­deo).
---

## ğŸ“Œ Parte 4 â€“ Gerando Respostas com GPT-4o

### ğŸ¯ Objetivo da Aula

* Mostrar como **usar um LLM (GPT-4o)** para **gerar respostas** baseadas em contexto recuperado de uma base de conhecimento.
* Complementa as etapas anteriores de *indexaÃ§Ã£o e recuperaÃ§Ã£o* com uma etapa de **geraÃ§Ã£o** (G da pipeline RAG â€“ Retrieval-Augmented Generation).

### âš™ï¸ Contexto Inicial

* AtÃ© aqui, o pipeline permite:

  * Fazer uma pergunta (ex: â€œO curso jÃ¡ comeÃ§ou? Ainda posso me inscrever?â€).
  * Recuperar documentos relevantes da base de conhecimento (via busca densa/simples).
* PrÃ³ximo passo: usar os documentos recuperados para **gerar uma resposta com um LLM**, incorporando o conteÃºdo ao prompt.

### ğŸ”§ ConfiguraÃ§Ã£o

* LLM escolhido: **GPT-4o** (via OpenAI API), por ser mais barato e rÃ¡pido que o GPT-4.
* API Key da OpenAI foi configurada via variÃ¡vel de ambiente.
* Cliente: `openai` importado e usado com a API `chat.completions`.

### âœ‰ï¸ Primeira tentativa simples

* Enviou apenas a pergunta do usuÃ¡rio no prompt:

  ```python
  messages = [{"role": "user", "content": question}]
  ```
* Resposta foi **genÃ©rica e superficial**, pois o modelo nÃ£o tinha acesso ao conteÃºdo da base de conhecimento.

### ğŸ§  Engenharia de Prompt (Prompt Engineering)

Para melhorar a resposta:

1. **Criou um template de prompt com papel (role):**

   * Exemplo de system message:

     ```python
     {"role": "system", "content": "VocÃª Ã© um assistente de ensino do curso."}
     ```

2. **InstruÃ§Ãµes adicionais no prompt:**

   * â€œUse **apenas os fatos do contexto** ao responder.â€
   * â€œSe o contexto nÃ£o contiver a resposta, diga isso explicitamente.â€

3. **CombinaÃ§Ã£o do contexto com a pergunta:**

   ```python
   prompt = f"""VocÃª Ã© um assistente de ensino do curso.
   Responda Ã  pergunta com base no conteÃºdo abaixo.
   Use apenas os fatos do contexto ao responder.

   CONTEXTO:
   {context}

   PERGUNTA:
   {question}
   """
   ```

### ğŸ—ƒï¸ ConstruÃ§Ã£o do Contexto

* Iterou sobre os documentos recuperados (`results`) e extraiu:

  * `section`, `question`, `answer` de cada documento.
  * Formatou como texto com quebras de linha para clareza.

Exemplo de estrutura:

```
SeÃ§Ã£o: IntroduÃ§Ã£o
Pergunta: Quando comeÃ§a o curso?
Resposta: O curso comeÃ§ou no dia 1Âº de maio.
```

### ğŸ¤– Envio do Prompt ao LLM

* O prompt formatado foi enviado ao `openai.ChatCompletion.create()` com `model="gpt-4o"`.
* Exemplo de resposta gerada:

  > "Sim, mesmo que o curso jÃ¡ tenha comeÃ§ado, vocÃª ainda pode se inscrever. Esteja atento aos prazos das tarefas e do projeto final."

### âœ… Resumo do Pipeline atÃ© aqui

1. **Entrada:** Pergunta do usuÃ¡rio.
2. **RecuperaÃ§Ã£o:** Busca por documentos relevantes na base de conhecimento.
3. **GeraÃ§Ã£o:** Prompt com contexto e pergunta Ã© enviado ao LLM, que gera a resposta.
4. **SaÃ­da:** Resposta final Ã© entregue ao usuÃ¡rio.

### ğŸ”„ PrÃ³ximos Passos

* Refatorar o cÃ³digo:
  * Modularizar: separar em funÃ§Ãµes reutilizÃ¡veis.
  * Facilitar a troca de componentes (ex: mudar o banco de dados ou o modelo LLM).
* Preparar a base para integrar com outras ferramentas de busca ou modelos de linguagem.
